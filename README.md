# ğŸš€ Text Generation using GPT-2

This project demonstrates text generation using the GPT-2 model from Hugging Face Transformers.

## ğŸ“Œ Project Overview

The notebook implements different text generation techniques:

- âœ… Greedy Search
- âœ… Beam Search
- âœ… Sampling (Top-k and Top-p)

The model generates text based on a given prompt using different decoding strategies.

## ğŸ› ï¸ Technologies Used

- Python
- PyTorch
- Hugging Face Transformers
- Google Colab

## ğŸ“‚ How It Works

1. Load pre-trained GPT-2 model
2. Tokenize input prompt
3. Generate text using:
   - Greedy Search
   - Beam Search
   - Sampling
4. Compare outputs

## ğŸ” Example Prompt

Artificial Intelligence

The model generates continuation text using different generation methods.

## ğŸ“Š Key Learning Outcomes

- Understanding text generation techniques
- Difference between deterministic and probabilistic decoding
- Working with transformer models

## ğŸ“ Run Instructions

1. Open the notebook in Google Colab
2. Install required libraries
3. Run all cells

## ğŸ‘¨â€ğŸ’» Author

Syed Ameer Sariya  
CSE Student
